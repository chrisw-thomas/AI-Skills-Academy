{"nbformat_minor": 1, "cells": [{"source": "# MNIST - The Interactive Version\nWe are going to build a simple model that we will run interactively in Watson Studio. This will not exploit Watson Machine Learning nor hardware acceleration through GPUs. There is a further lab that you can work on that does this if we have time or you want to complete it in your own time.\n\n## Tensor Flow\nOur model will be built using Tensor Flow, a free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and used for machine learning applications such as neural networks. It is used for both research and production at Google. It is one of the most commonly used technologies for machine learning.\n\nTensorFlow was originally developed for internal use within Google use but has now been released under the Apache 2.0 open-source license.\n\n## Keras\nTo keep things simple we will be using Keras, an open-source neural-network library written in Python. It runs on top of TensorFlow and is designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible.\n\nThe first thing we must do is ingest our training data. The MNIST dataset is so popular that it is actually built in to Keras!\n\nRun the following cell to load the MNIST dataset into some variables - one pair for training and the other for testing.", "cell_type": "markdown", "metadata": {}}, {"source": "from keras.datasets import mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [], "execution_count": null}, {"source": "We have just loaded the training image data into the x arrays and the labels for that training data into the y arrays. Similarly, the test image data and the test label data has also been loaded.\n\nTo see what the training and test data looks like execute the following cells. These will display the \"shape\" of the training and test data.", "cell_type": "markdown", "metadata": {}}, {"source": "print('Training: ' + str(x_train.shape))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "print('Test: ' + str(x_test.shape))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "We can see that the training data consists of a 3-dimensional array in which are stored 60,000 images, where each image is 28 pixels wide and 28 pixels high.\nThe test data is the same - but we only have 10,000 images.\n\nLet's have a look at one of these images to see what it looks like... first the label", "cell_type": "markdown", "metadata": {}}, {"source": "import matplotlib.pyplot as plt\nimage_index = 1234 # You may use any number from 0 to 59999\nprint ('Label: ' + str(y_train[image_index]))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Now the training image itself...", "cell_type": "markdown", "metadata": {}}, {"source": "plt.imshow(x_train[image_index], cmap='Greys')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "We now need to massage the data a little to use it with Keras and prepare it for training the model.\n\nAt the end we will print out the array of pixel values for one of the training images... see if you can work out what number it represents!", "cell_type": "markdown", "metadata": {}}, {"source": "# Reshape the array to 4-dimensions so that it can work with the Keras API\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\ninput_shape = (28, 28, 1)\n\n# Make sure that the image pixel values are floating point numbers and not integers so that we can get decimal points after division. This improves accuracy\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\n\n# Normalise the RGB values by dividing them by the maximum RGB value.\nx_train /= 255\nx_test /= 255\n\nprint('x_train shape:', x_train.shape)\nprint('Number of images in x_train', x_train.shape[0])\nprint('Number of images in x_test', x_test.shape[0])\nprint(str(x_train[123]))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Now we will import some of the Python modules for Keras and Tensor Flow and define some things for the model that we want to train", "cell_type": "markdown", "metadata": {}}, {"source": "# Import the required Keras modules containing model and layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\nimport tensorflow as tf\n\nmodel = Sequential()\n\nmodel.add(Conv2D(28, kernel_size=(3, 3), input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten()) # Flattening the 2D arrays for fully connected layers\nmodel.add(Dense(128, activation=tf.nn.relu))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation=tf.nn.softmax))\n\nprint('Done')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "##### The following statements define the learning process (compile) and then begins to fit the model to our training data.\n\nOne epoch consists of a full training cycle on the training set. Once every image in the set has been evaluated the epoch has been completed. A model will require several epochs to achieve the required level of accuracy.\n\nNormally you would continued for as long as it takes to achieve the required accuracy or for a specific number of epochs. For our workshop we will run 5 epochs.\n\nAs each epoch is being run you will see a progress bar and the loss and accuracy achieved will be shown. What you should observe is that the loss reduces and the accuracy increases for each epoch.", "cell_type": "markdown", "metadata": {}}, {"source": "model.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy', \n              metrics=['accuracy'])\n\nmodel.fit(x=x_train,y=y_train, epochs=5)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Now we have built a model we can evaluate it using our test data.\n\nThe two numbers produced will be the loss and the accuracy achieved.", "cell_type": "markdown", "metadata": {}}, {"source": "model.evaluate(x_test, y_test)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Let's test the model with some specific images and see if it interprets them correctly.\n\nYou can set the image index to any number from 0 to 9999.", "cell_type": "markdown", "metadata": {}}, {"source": "image_index = 9874\nplt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\npred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\nprint(\"I predict: \",pred.argmax())", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}
